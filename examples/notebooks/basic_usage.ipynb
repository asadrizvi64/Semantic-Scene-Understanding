{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Narrative Scene Understanding - Basic Usage\n",
        "\n",
        "This notebook demonstrates the basic usage of the Narrative Scene Understanding system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path to import from modules\n",
        "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
        "\n",
        "# Import core functionality\n",
        "from narrative_scene_understanding import process_video, query_scene\n",
        "from modules.utils import visualize_graph"
      ],
      "outputs": [],
      "id": "first-import-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "\n",
        "First, let's set up the configuration for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "source": [
        "# Define the configuration\n",
        "config = {\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"output_dir\": \"../outputs/notebook_output\",\n",
        "    \"frame_rate\": 2.0,  # Extract 2 frames per second\n",
        "    \"adaptive_sampling\": True,  # Use adaptive sampling based on motion\n",
        "    \"max_frames\": 300,  # Limit for demonstration\n",
        "    # Configure LLM to use Ollama\n",
        "    \"query_engine\": {\n",
        "        \"use_ollama\": True,\n",
        "        \"ollama_model\": \"llama3\",  # Or any other model you have in Ollama\n",
        "        \"ollama_url\": \"http://localhost:11434\",\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
        "\n",
        "# Path to the video file\n",
        "video_path = \"../videos/movie_scene.mp4\"\n",
        "\n",
        "# Check if video exists\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"Video file not found: {video_path}\")\n",
        "    print(\"Please download example videos or update the path to your video file.\")"
      ],
      "outputs": [],
      "id": "config-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Process the Video\n",
        "\n",
        "Now, let's process the video to extract narrative understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "source": [
        "# Process the video\n",
        "results = process_video(video_path, config)\n",
        "\n",
        "# Print the summary\n",
        "print(\"\\n===== SCENE SUMMARY =====\")\n",
        "print(results[\"summary\"])"
      ],
      "outputs": [],
      "id": "process-video-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visualize the Knowledge Graph\n",
        "\n",
        "Let's visualize the knowledge graph to better understand the scene structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "source": [
        "# Visualize the graph\n",
        "graph = results[\"graph\"]\n",
        "vis_path = os.path.join(config[\"output_dir\"], \"graph_visualization.png\")\n",
        "visualize_graph(graph, vis_path)\n",
        "\n",
        "# Display the image\n",
        "from IPython.display import Image\n",
        "Image(filename=vis_path)"
      ],
      "outputs": [],
      "id": "visualize-graph-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Query the Scene\n",
        "\n",
        "Now, let's ask some questions about the scene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "source": [
        "# Define a question\n",
        "question = \"Why does Character A appear nervous during the conversation?\"\n",
        "\n",
        "# Query the scene\n",
        "response = query_scene(question, results=results)\n",
        "\n",
        "# Print the answer\n",
        "print(\"\\n===== QUESTION =====\")\n",
        "print(question)\n",
        "print(\"\\n===== ANSWER =====\")\n",
        "print(response[\"answer\"])\n",
        "print(\"\\n===== REASONING PROCESS =====\")\n",
        "print(response[\"reasoning\"])"
      ],
      "outputs": [],
      "id": "query-scene-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore Character Emotions\n",
        "\n",
        "Let's analyze the emotional arcs of characters in the scene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Emotion to valence mapping (same as in the system)\n",
        "EMOTION_VALENCE = {\n",
        "    \"happy\": 1.0, \"content\": 0.6, \"neutral\": 0.0, \"concerned\": -0.4,\n",
        "    \"nervous\": -0.5, \"sad\": -0.6, \"angry\": -0.8, \"surprised\": 0.3,\n",
        "    \"distressed\": -0.7, \"confused\": -0.3\n",
        "}\n",
        "\n",
        "def emotion_to_valence(emotion):\n",
        "    emotion = emotion.lower()\n",
        "    if emotion in EMOTION_VALENCE:\n",
        "        return EMOTION_VALENCE[emotion]\n",
        "    # Try to find partial match\n",
        "    for key, value in EMOTION_VALENCE.items():\n",
        "        if key in emotion or emotion in key:\n",
        "            return value\n",
        "    return 0.0\n",
        "\n",
        "# Create figure\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot emotional arcs for each character\n",
        "for char_id, char_info in results[\"analysis\"][\"characters\"].items():\n",
        "    char_name = char_info.get(\"name\", char_id)\n",
        "    emotions = char_info.get(\"emotions\", [])\n",
        "    \n",
        "    if emotions:\n",
        "        timestamps = [e[\"timestamp\"] for e in emotions]\n",
        "        valence = [emotion_to_valence(e[\"emotion\"]) * e.get(\"intensity\", 1.0) for e in emotions]\n",
        "        \n",
        "        plt.plot(timestamps, valence, 'o-', label=char_name)\n",
        "        \n",
        "        # Annotate emotions\n",
        "        for t, v, e in zip(timestamps, valence, emotions):\n",
        "            plt.annotate(e[\"emotion\"], (t, v), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
        "\n",
        "# Set up plot\n",
        "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)  # Neutral line\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Emotional Valence\")\n",
        "plt.title(\"Character Emotional Arcs\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(-1.1, 1.1)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save and display\n",
        "plt.savefig(os.path.join(config[\"output_dir\"], \"emotional_arcs.png\"))\n",
        "plt.show()"
      ],
      "outputs": [],
      "id": "character-emotions-cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Results\n",
        "\n",
        "Finally, let's save the full results to a JSON file for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "source": [
        "# Save results to JSON\n",
        "output_path = os.path.join(config[\"output_dir\"], \"full_analysis.json\")\n",
        "\n",
        "# Convert the NetworkX graph to a serializable format\n",
        "serializable_results = results.copy()\n",
        "serializable_results[\"graph\"] = {\n",
        "    \"nodes\": [\n",
        "        {\"id\": node, **{k: v for k, v in attrs.items() if isinstance(v, (str, int, float, bool, list, dict))}}\n",
        "        for node, attrs in graph.nodes(data=True)\n",
        "    ],\n",
        "    \"edges\": [\n",
        "        {\"source\": u, \"target\": v, **{k: v for k, v in attrs.items() if isinstance(v, (str, int, float, bool, list, dict))}}\n",
        "        for u, v, attrs in graph.edges(data=True)\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(serializable_results, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {output_path}\")"
      ],
      "outputs": [],
      "id": "save-results-cell"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}